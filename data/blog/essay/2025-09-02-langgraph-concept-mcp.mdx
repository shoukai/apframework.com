---
title: 'LangGraph é›†æˆ FastMCP 2.0 åŸºç¡€æ•™ç¨‹ï¼šä¸¤ç§é›†æˆæ–¹å¼çš„å®Œæ•´å®è·µ'
date: '2025-09-02'
tags: ['LangGraph', 'FastMCP', 'MCP', 'Agent', 'Python', 'å·¥å…·é›†æˆ', 'æ¨¡å‹ä¸Šä¸‹æ–‡åè®®', 'æ•™ç¨‹']
draft: false
summary: 'æœ¬æ–‡ä½œä¸ºåŸºç¡€æ•™ç¨‹ï¼Œç³»ç»Ÿè®²è§£ LangGraph ä¸ FastMCP 2.0ï¼ˆMCP 2.0ï¼‰ çš„ä¸¤ç§é›†æˆæ–¹å¼ï¼šä½œä¸ºå®¢æˆ·ç«¯åœ¨ LangGraph å·¥ä½œæµä¸­è°ƒç”¨ MCP å·¥å…·ï¼Œä»¥åŠå°†ç°æœ‰ LangGraph Agent å°è£…ä¸º FastMCP æœåŠ¡å™¨å¯¹å¤–æä¾›æœåŠ¡ã€‚å†…å®¹åŒ…å«ç¯å¢ƒå‡†å¤‡ã€ç¤ºä¾‹ä»£ç ã€è¿è¡ŒéªŒè¯ä¸ MCP Inspector è°ƒè¯•ï¼Œå¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹å¹¶å®Œæˆç«¯åˆ°ç«¯æ‰“é€šã€‚'
---

<div className="text-center">
  <img src="/static/images/2025-09-02-langgraph-concept-mcp/fatos-bytyqi-Agx5_TLsIf4-unsplash.jpg" alt="Code" />
  <p>Code in LangGraphï¼ˆPhoto by <a href="https://unsplash.com/@fatosi?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Fatos Bytyqi</a> on <a href="https://unsplash.com/photos/gray-laptop-computer-on-brown-wooden-desk-Agx5_TLsIf4?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>)
ï¼‰</p>
</div>

LangGraph ä¸ FastMCP 2.0ï¼ˆMCP 2.0ï¼‰çš„ç»“åˆï¼Œä¸»è¦æœ‰ä¸¤ç§å…¸å‹æ–¹å¼ï¼š

* å®¢æˆ·ç«¯é›†æˆï¼šå°† FastMCP æœåŠ¡å™¨æš´éœ²çš„å·¥å…·ã€æç¤ºä¸èµ„æºï¼Œä½œä¸º LangGraph å·¥ä½œæµä¸­çš„å¯è°ƒç”¨èŠ‚ç‚¹ï¼ˆæœ€å¸¸è§ï¼‰ã€‚
* æœåŠ¡ç«¯å°è£…ï¼šæŠŠå·²æœ‰çš„ LangGraph Agent å°è£…ä¸º FastMCP æœåŠ¡å™¨ï¼Œå¯¹å¤–ä»¥æ ‡å‡†åè®®æä¾›èƒ½åŠ›ï¼ˆåå‘é›†æˆï¼‰ã€‚

## å®¢æˆ·ç«¯é›†æˆ

### MCP æ¨¡æ‹ŸæœåŠ¡

ç”¨äºæœ¬åœ°æµ‹è¯•ä¸è°ƒè¯•çš„ MCP 2.0 æ¨¡æ‹ŸæœåŠ¡ã€‚

ä»¥ä¸‹ç¤ºä¾‹åŸºäº FastMCP 2.0 ç¼–å†™ã€‚

```Shell
uv add fastmcp
```

#### ç¤ºä¾‹ä»£ç 

æœ€åŸºç¡€çš„ç®—æœ¯å·¥å…·ï¼š

```Python
from fastmcp import FastMCP

mcp = FastMCP("MyServer")

@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b

@mcp.tool()
def multiply(a: int, b: int) -> int:
    """Multiply two numbers"""
    return a * b

if __name__ == "__main__":
    # Start an HTTP server on port 8000
    mcp.run(transport="http", host="127.0.0.1", port=8000)
```

```Shell
uv run concepts-mcp/mcp_server_sample.py
```

è¿è¡Œåï¼Œæ§åˆ¶å°å°†è¾“å‡ºï¼š


```Shell
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                            â”‚
â”‚        _ __ ___  _____           __  __  _____________    ____    ____     â”‚
â”‚       _ __ ___ .'____/___ ______/ /_/  |/  / ____/ __ \  |___ \  / __ \    â”‚
â”‚      _ __ ___ / /_  / __ `/ ___/ __/ /|_/ / /   / /_/ /  ___/ / / / / /    â”‚
â”‚     _ __ ___ / __/ / /_/ (__  ) /_/ /  / / /___/ ____/  /  __/_/ /_/ /     â”‚
â”‚    _ __ ___ /_/    \____/____/\__/_/  /_/\____/_/      /_____(*)____/      â”‚
â”‚                                                                            â”‚
â”‚                                                                            â”‚
â”‚                                FastMCP  2.0                                â”‚
â”‚                                                                            â”‚
â”‚                                                                            â”‚
â”‚               ğŸ–¥ï¸  Server name:     MyServer                                 â”‚
â”‚               ğŸ“¦ Transport:       Streamable-HTTP                          â”‚
â”‚               ğŸ”— Server URL:      http://127.0.0.1:8000/mcp                â”‚
â”‚                                                                            â”‚
â”‚               ğŸï¸  FastMCP version: 2.12.0                                   â”‚
â”‚               ğŸ¤ MCP SDK version: 1.13.1                                   â”‚
â”‚                                                                            â”‚
â”‚               ğŸ“š Docs:            https://gofastmcp.com                    â”‚
â”‚               ğŸš€ Deploy:          https://fastmcp.cloud                    â”‚
â”‚                                                                            â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯


[09/01/25 22:54:03] INFO     Starting MCP server 'MyServer' with transport 'http' on                      server.py:1571
                             http://127.0.0.1:8000/mcp
INFO:     Started server process [24590]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
```

#### éªŒè¯åŠŸèƒ½

å®‰è£…å¹¶å¯åŠ¨ MCP Inspector
ä½¿ç”¨ npm å®‰è£… MCP Inspectorï¼š
```Shell
npm install -g @modelcontextprotocol/inspector
```
æˆ–è€…ï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨npxè¿è¡ŒInspectorï¼Œæ— éœ€å…¨å±€å®‰è£…ï¼š
```Shell
npx @modelcontextprotocol/inspector
```
å¯åŠ¨åï¼ŒInspector å°†åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ï¼Œé»˜è®¤åœ°å€ä¸º http://127.0.0.1:6274ã€‚

```Shell
npx @modelcontextprotocol/inspector
Starting MCP inspector...
âš™ï¸ Proxy server listening on 127.0.0.1:6277
ğŸ”‘ Session token: 668f8a6edafccaa8a276b5bcfbfb1d4e0fe486b14b5b42208d11d777cd7f17b4
Use this token to authenticate requests or set DANGEROUSLY_OMIT_AUTH=true to disable auth

ğŸ”— Open inspector with token pre-filled:
   http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=668f8a6edafccaa8a276b5bcfbfb1d4e0fe486b14b5b42208d11d777cd7f17b4

ğŸ” MCP Inspector is up and running at http://127.0.0.1:6274 ğŸš€
```

æ¨¡æ‹ŸæœåŠ¡å¯åŠ¨åï¼ŒInspector ä¼šè‡ªåŠ¨å‘ç°å¹¶å±•ç¤º MyServer æœåŠ¡å™¨çš„å…¨éƒ¨å·¥å…·ã€‚

![MCP Inspector](/static/images/2025-09-02-langgraph-concept-mcp/iShot_2025-09-01_22.58.19.png)

ç”¨äºå¿«é€ŸéªŒè¯å·¥å…·å¯ç”¨æ€§ï¼Œé¿å…é›†æˆè¿‡ç¨‹ä¸­çš„å¹²æ‰°é¡¹ã€‚

### LangGraph è°ƒç”¨ MCP æœåŠ¡

æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰æ˜¯ä¸€é¡¹å¼€æ”¾æ ‡å‡†ï¼Œç”¨äºè§„èŒƒåº”ç”¨å¦‚ä½•å‘è¯­è¨€æ¨¡å‹æä¾›å·¥å…·ä¸ä¸Šä¸‹æ–‡ã€‚å€ŸåŠ© langchain-mcp-adaptersï¼ŒLangGraph æ™ºèƒ½ä½“å¯ç›´æ¥ä½¿ç”¨ MCP æœåŠ¡å™¨ä¸Šå®šä¹‰çš„å·¥å…·ã€‚

<div className="text-center">
  <img src="/static/images/2025-09-02-langgraph-concept-mcp/mcp.png" alt="BAML" />
  <p>æ¨¡å‹ä¸Šä¸‹æ–‡åè®® (MCP) ï¼ˆå›¾ç‰‡æºè‡ª [LangGraph](http://localhost:3000/blog/essay/2025-09-02-langgraph-concept-mcp) ï¼‰</p>
</div>

å®‰è£… langchain-mcp-adaptersï¼Œä½¿ LangGraph èƒ½è°ƒç”¨ MCP å·¥å…·ã€‚

```Shell
uv add langchain-mcp-adapters
```

```Shell
uv add langchain langchain-openai langchain-deepseek langgraph python-dotenv 
```

å·¥ç¨‹æ ¹ç›®å½•æ·»åŠ   `.env` æ–‡ä»¶

```
DEEPSEEK_API_KEY=sk-â€¦â€¦
```

ç¼–å†™ä»£ç  langgraph_use_mcp_as_client.pyï¼š

```Python
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode

import os
from dotenv import load_dotenv

# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# Initialize the model
model = init_chat_model(
    "deepseek-chat",  # ä½¿ç”¨DeepSeekæ¨¡å‹
    api_key=os.environ.get("DEEPSEEK_API_KEY")
)

# Set up MCP client
client = MultiServerMCPClient(
    {
        "math": {
            # make sure you start your math server on port 8000
            "url": "http://127.0.0.1:8000/mcp/",
            "transport": "streamable_http",
        }
    }
)

async def main():
    # Get tools from MCP server
    print("\n=== è·å–MCPå·¥å…· ===")
    tools = await client.get_tools()
    print(f"å¯ç”¨å·¥å…·: {[tool.name for tool in tools]}")
    for tool in tools:
        print(f"  - {tool.name}: {tool.description}")
    
    # Bind tools to model
    print("\n=== ç»‘å®šå·¥å…·åˆ°æ¨¡å‹ ===")
    model_with_tools = model.bind_tools(tools)
    print(f"å·²å°† {len(tools)} ä¸ªå·¥å…·ç»‘å®šåˆ°æ¨¡å‹")
    
    # Create ToolNode
    tool_node = ToolNode(tools)
    
    def should_continue(state: MessagesState):
        messages = state["messages"]
        last_message = messages[-1]
        if last_message.tool_calls:
            return "tools"
        return END
    
    # Define call_model function
    async def call_model(state: MessagesState):
        messages = state["messages"]
        print("\n=== è°ƒç”¨LLMæ¨¡å‹ ===")
        print(f"è¾“å…¥æ¶ˆæ¯æ•°é‡: {len(messages)}")
        if messages:
            print(f"æœ€æ–°æ¶ˆæ¯: {messages[-1].content if hasattr(messages[-1], 'content') else str(messages[-1])}")
        
        response = await model_with_tools.ainvoke(messages)
        print(f"æ¨¡å‹å“åº”ç±»å‹: {type(response).__name__}")
        if hasattr(response, 'content'):
            print(f"å“åº”å†…å®¹: {response.content}")
        if hasattr(response, 'tool_calls') and response.tool_calls:
            print(f"å·¥å…·è°ƒç”¨: {len(response.tool_calls)} ä¸ª")
            for i, tool_call in enumerate(response.tool_calls):
                print(f"  å·¥å…· {i+1}: {tool_call['name']} - å‚æ•°: {tool_call['args']}")
        
        return {"messages": [response]}
    
    # Build the graph
    print("\n=== æ„å»ºLangGraphå·¥ä½œæµ ===")
    builder = StateGraph(MessagesState)
    builder.add_node("call_model", call_model)
    builder.add_node("tools", tool_node)
    print("å·²æ·»åŠ èŠ‚ç‚¹: call_model (æ¨¡å‹è°ƒç”¨) å’Œ tools (å·¥å…·æ‰§è¡Œ)")
    
    builder.add_edge(START, "call_model")
    builder.add_conditional_edges(
        "call_model",
        should_continue,
    )
    builder.add_edge("tools", "call_model")
    
    # Compile the graph
    graph = builder.compile()
    
    # Test the graph
    print("\n=== å¼€å§‹æµ‹è¯•æ•°å­¦è®¡ç®— ===")
    test_question = "what's (3 + 5) x 12?"
    print(f"æµ‹è¯•é—®é¢˜: {test_question}")
    
    math_response = await graph.ainvoke(
        {"messages": [{"role": "user", "content": test_question}]}
    )
    
    print("\n=== æœ€ç»ˆç»“æœ ===")
    print(f"æ¶ˆæ¯é“¾é•¿åº¦: {len(math_response['messages'])}")
    for i, msg in enumerate(math_response['messages']):
        msg_type = type(msg).__name__
        if hasattr(msg, 'content'):
            print(f"æ¶ˆæ¯ {i+1} ({msg_type}): {msg.content}")
        else:
            print(f"æ¶ˆæ¯ {i+1} ({msg_type}): {str(msg)}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

```


### æµ‹è¯•ç»“æœ

MCP æœåŠ¡å™¨ï¼šå·²åœ¨ http://127.0.0.1:8000/mcp/ æˆåŠŸè¿è¡Œï¼Œæä¾› add ä¸ multiply å·¥å…·ã€‚

MCP å®¢æˆ·ç«¯ï¼šå·²æˆåŠŸè¿æ¥åˆ°æœåŠ¡å™¨å¹¶å®Œæˆæ•°å­¦è®¡ç®—æµ‹è¯•ã€‚

- é—®é¢˜ï¼š"what's (3 + 5) x 12?"
- ç»“æœï¼šæ­£ç¡®è®¡ç®—å‡ºç­”æ¡ˆ 96
- è¿‡ç¨‹ï¼šå…ˆè°ƒç”¨ add(3, 5) å¾—åˆ° 8ï¼Œå†è°ƒç”¨ multiply(8, 12) å¾—åˆ° 96

### å®é™…è¿è¡Œæ•ˆæœ
é€šè¿‡è¿è¡Œæµ‹è¯•ï¼Œå¯çœ‹åˆ°å®Œæ•´çš„è®¡ç®—æµç¨‹ï¼š

1. é—®é¢˜è¾“å…¥ï¼š"what's (3 + 5) x 12?"
2. ç¬¬ä¸€æ¬¡ LLM è°ƒç”¨ï¼šæ¨¡å‹å†³å®šå…ˆè®¡ç®—åŠ æ³•ï¼Œè°ƒç”¨ add(3, 5)
3. å·¥å…·æ‰§è¡Œï¼šMCP æœåŠ¡å™¨è¿”å›ç»“æœ 8
4. ç¬¬äºŒæ¬¡ LLM è°ƒç”¨ï¼šæ¨¡å‹ç»§ç»­è®¡ç®—ä¹˜æ³•ï¼Œè°ƒç”¨ multiply(8, 12)
5. å·¥å…·æ‰§è¡Œï¼šMCP æœåŠ¡å™¨è¿”å›ç»“æœ 96
6. ç¬¬ä¸‰æ¬¡ LLM è°ƒç”¨ï¼šæ¨¡å‹æ€»ç»“æœ€ç»ˆç­”æ¡ˆ


```Shell
=== è·å–MCPå·¥å…· ===
å¯ç”¨å·¥å…·: ['add', 'multiply']
  - add: Add two numbers
  - multiply: Multiply two numbers

=== ç»‘å®šå·¥å…·åˆ°æ¨¡å‹ ===
å·²å°† 2 ä¸ªå·¥å…·ç»‘å®šåˆ°æ¨¡å‹

=== æ„å»ºLangGraphå·¥ä½œæµ ===
å·²æ·»åŠ èŠ‚ç‚¹: call_model (æ¨¡å‹è°ƒç”¨) å’Œ tools (å·¥å…·æ‰§è¡Œ)

=== å¼€å§‹æµ‹è¯•æ•°å­¦è®¡ç®— ===
æµ‹è¯•é—®é¢˜: what's (3 + 5) x 12?

=== è°ƒç”¨LLMæ¨¡å‹ ===
è¾“å…¥æ¶ˆæ¯æ•°é‡: 1
æœ€æ–°æ¶ˆæ¯: what's (3 + 5) x 12?
æ¨¡å‹å“åº”ç±»å‹: AIMessage
å“åº”å†…å®¹: I'll help you calculate (3 + 5) Ã— 12. Let me break this down step by step.
å·¥å…·è°ƒç”¨: 1 ä¸ª
  å·¥å…· 1: add - å‚æ•°: {'a': 3, 'b': 5}

=== è°ƒç”¨LLMæ¨¡å‹ ===
è¾“å…¥æ¶ˆæ¯æ•°é‡: 3
æœ€æ–°æ¶ˆæ¯: 8
æ¨¡å‹å“åº”ç±»å‹: AIMessage
å“åº”å†…å®¹: Now I'll multiply the result (8) by 12:
å·¥å…·è°ƒç”¨: 1 ä¸ª
  å·¥å…· 1: multiply - å‚æ•°: {'a': 8, 'b': 12}

=== è°ƒç”¨LLMæ¨¡å‹ ===
è¾“å…¥æ¶ˆæ¯æ•°é‡: 5
æœ€æ–°æ¶ˆæ¯: 96
æ¨¡å‹å“åº”ç±»å‹: AIMessage
å“åº”å†…å®¹: The result of (3 + 5) Ã— 12 is **96**. 

Here's the calculation:
- First, 3 + 5 = 8
- Then, 8 Ã— 12 = 96

=== æœ€ç»ˆç»“æœ ===
æ¶ˆæ¯é“¾é•¿åº¦: 6
æ¶ˆæ¯ 1 (HumanMessage): what's (3 + 5) x 12?
æ¶ˆæ¯ 2 (AIMessage): I'll help you calculate (3 + 5) Ã— 12. Let me break this down step by step.
æ¶ˆæ¯ 3 (ToolMessage): 8
æ¶ˆæ¯ 4 (AIMessage): Now I'll multiply the result (8) by 12:
æ¶ˆæ¯ 5 (ToolMessage): 96
æ¶ˆæ¯ 6 (AIMessage): The result of (3 + 5) Ã— 12 is **96**. 

Here's the calculation:
- First, 3 + 5 = 8
- Then, 8 Ã— 12 = 96
```

## æœåŠ¡ç«¯å°è£…

### æ–¹æ¡ˆæ¦‚è§ˆ
æœ¬èŠ‚æ¼”ç¤ºå¦‚ä½•å°†å·²æœ‰çš„ LangGraph å·¥ä½œæµå°è£…ä¸ºä¸€ä¸ª FastMCP æœåŠ¡å™¨ï¼Œå¯¹å¤–ä»¥ MCP æ ‡å‡†åè®®æä¾›èƒ½åŠ›ã€‚
æ ¸å¿ƒæ€è·¯æ˜¯ï¼šåœ¨ FastMCP ä¸­æ³¨å†Œä¸€ä¸ªå·¥å…·ï¼ˆprocess_text_with_langgraphï¼‰ï¼Œå…¶å†…éƒ¨è°ƒç”¨ LangGraph çš„å·¥ä½œæµï¼Œå®ç°â€œé¢„å¤„ç† â†’ AI åˆ†æ â†’ ç»“æœæ±‡æ€»â€çš„ç«¯åˆ°ç«¯å¤„ç†ã€‚

ç¤ºä¾‹ä»£ç ï¼šlanggraph_use_mcp_as_server.py

```Python
#!/usr/bin/env python3
"""
ç®€åŒ–çš„ FastMCP + LangGraph æ¼”ç¤º
å±•ç¤ºå¦‚ä½•åœ¨FastMCPä¸­é›†æˆLangGraphå·¥ä½œæµ
"""

import asyncio
import os
from typing import TypedDict, List
from datetime import datetime

from fastmcp import FastMCP, Context
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage
from langchain_deepseek import ChatDeepSeek
from langchain_core.prompts import ChatPromptTemplate

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

# å®šä¹‰ç®€å•çš„çŠ¶æ€ç±»å‹
class TextProcessState(TypedDict):
    input_text: str
    processed_text: str
    ai_response: str
    steps: List[str]

# åˆå§‹åŒ–DeepSeekæ¨¡å‹
model = ChatDeepSeek(
    model="deepseek-chat",
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    temperature=0.7
)

# åˆ›å»ºFastMCPå®ä¾‹
mcp = FastMCP("Simple-FastMCP-LangGraph")

def create_text_processing_graph():
    """åˆ›å»ºæ–‡æœ¬å¤„ç†çš„LangGraphå·¥ä½œæµ"""
    
    async def preprocess_text(state: TextProcessState) -> TextProcessState:
        """é¢„å¤„ç†æ–‡æœ¬"""
        input_text = state["input_text"]
        
        # ç®€å•çš„é¢„å¤„ç†ï¼šå»é™¤å¤šä½™ç©ºæ ¼ï¼Œæ·»åŠ æ—¶é—´æˆ³
        processed = input_text.strip()
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        return {
            **state,
            "processed_text": processed,
            "steps": state["steps"] + [f"æ–‡æœ¬é¢„å¤„ç†å®Œæˆ ({timestamp})"]
        }
    
    async def generate_ai_response(state: TextProcessState) -> TextProcessState:
        """ç”ŸæˆAIå“åº”"""
        processed_text = state["processed_text"]
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æåŠ©æ‰‹ã€‚è¯·å¯¹ç”¨æˆ·æä¾›çš„æ–‡æœ¬è¿›è¡Œåˆ†æå’Œæ€»ç»“ï¼Œæä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚"),
            ("human", "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼š\n\n{text}\n\nè¯·æä¾›ï¼š1) ä¸»è¦å†…å®¹æ€»ç»“ 2) å…³é”®ä¿¡æ¯æå– 3) ç®€çŸ­è¯„ä»·")
        ])
        
        try:
            response = await model.ainvoke(prompt.format_messages(text=processed_text))
            ai_content = response.content
        except Exception as e:
            ai_content = f"AIå¤„ç†å‡ºé”™: {str(e)}"
        
        return {
            **state,
            "ai_response": ai_content,
            "steps": state["steps"] + ["AIåˆ†æå®Œæˆ"]
        }
    
    # æ„å»ºå·¥ä½œæµå›¾
    workflow = StateGraph(TextProcessState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("preprocess", preprocess_text)
    workflow.add_node("ai_analyze", generate_ai_response)
    
    # æ·»åŠ è¾¹
    workflow.add_edge(START, "preprocess")
    workflow.add_edge("preprocess", "ai_analyze")
    workflow.add_edge("ai_analyze", END)
    
    # ç¼–è¯‘å›¾
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)

# åˆ›å»ºå…¨å±€çš„LangGraphå®ä¾‹
text_processor = create_text_processing_graph()

@mcp.tool()
async def process_text_with_langgraph(text: str, ctx: Context = None) -> str:
    """
    ä½¿ç”¨LangGraphå¤„ç†æ–‡æœ¬
    
    Args:
        text: è¦å¤„ç†çš„æ–‡æœ¬å†…å®¹
    
    Returns:
        å¤„ç†ç»“æœ
    """
    return await _analyze_text(text, ctx)

# åŸå§‹å·¥å…·å‡½æ•°ï¼ˆä¸ä½¿ç”¨è£…é¥°å™¨ï¼‰
async def _analyze_text(text: str, ctx = None) -> str:
    """å†…éƒ¨æ–‡æœ¬åˆ†æå‡½æ•°"""
    if ctx:
        await ctx.info(f"å¼€å§‹åˆ†ææ–‡æœ¬: {text[:30]}...")
    
    try:
        # åˆå§‹çŠ¶æ€
        initial_state = {
            "input_text": text,
            "processed_text": "",
            "ai_response": "",
            "steps": []
        }
        
        # é…ç½®
        config = {
            "configurable": {
                "thread_id": f"analyze_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
            }
        }
        
        if ctx:
            await ctx.info("æ‰§è¡ŒLangGraphå·¥ä½œæµ...")
        
        # è¿è¡Œå·¥ä½œæµ
        final_state = await text_processor.ainvoke(initial_state, config)
        
        if ctx:
            await ctx.info("åˆ†æå®Œæˆ")
        
        # æ ¼å¼åŒ–ç»“æœ
        result = f"""ğŸ“Š æ–‡æœ¬åˆ†æç»“æœ

ğŸ“ åŸå§‹æ–‡æœ¬:
{final_state['input_text']}

ğŸ¤– AIåˆ†æ:
{final_state['ai_response']}

âš™ï¸ å¤„ç†æ­¥éª¤:
{' â†’ '.join(final_state['steps'])}

â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
        
        return result
        
    except Exception as e:
        error_msg = f"æ–‡æœ¬åˆ†æå¤±è´¥: {str(e)}"
        if ctx:
            await ctx.error(error_msg)
        return error_msg

if __name__ == "__main__":
    import sys
    
    # æœåŠ¡å™¨æ¨¡å¼
    print("ğŸš€ å¯åŠ¨ Simple FastMCP + LangGraph æœåŠ¡å™¨")
    print("ğŸ“‹ å¯ç”¨å·¥å…·:")
    print("   - process_text_with_langgraph: ä½¿ç”¨LangGraphå¤„ç†æ–‡æœ¬")
    print("ğŸŒ æœåŠ¡å™¨åœ°å€: http://127.0.0.1:8004/mcp")
    print("ğŸ’¡ æµ‹è¯•å‘½ä»¤: python simple_fastmcp_demo.py --test")
    print("=" * 60)
    
    mcp.run(
        transport="http",
        host="127.0.0.1",
        port=8004,
        log_level="info"
    )

å¯åŠ¨ä¸éªŒè¯
åœ¨å¯åŠ¨å‰ï¼Œè¯·ç¡®ä¿å·²åœ¨é¡¹ç›®æ ¹ç›®å½•é…ç½® .env å¹¶è®¾ç½® DEEPSEEK_API_KEYã€‚

è¿è¡Œä»£ç 
```Shell
uv run langgraph_use_mcp_as_server.py
```

å¯åŠ¨æˆåŠŸåï¼Œå¯ä½¿ç”¨ MCP Inspector è¿æ¥ http://127.0.0.1:8004/mcpï¼Œç•Œé¢ä¸­å°†è‡ªåŠ¨å‘ç°å¹¶å±•ç¤ºå·¥å…· process_text_with_langgraphã€‚
é€‰æ‹©è¯¥å·¥å…·ï¼Œè¾“å…¥ä»»æ„æ–‡æœ¬ï¼ˆä¾‹å¦‚â€œè¯·åˆ†æè¿™æ®µå…³äº LangGraph ä¸ FastMCP é›†æˆçš„æè¿°ï¼Œç»™å‡ºè¦ç‚¹ä¸å»ºè®®â€ï¼‰è¿›è¡Œè°ƒç”¨ã€‚
å·¥å…·å°†è¿”å›ç»“æ„åŒ–çš„æ–‡æœ¬ç»“æœï¼ŒåŒ…æ‹¬ï¼š
- åŸå§‹æ–‡æœ¬ï¼šå›æ˜¾ä½ è¾“å…¥çš„å†…å®¹ï¼›
- AI åˆ†æï¼šæ¨¡å‹åŸºäºæç¤ºæ¨¡ç‰ˆç»™å‡ºçš„æ€»ç»“ã€å…³é”®ä¿¡æ¯ä¸è¯„ä»·ï¼›
- å¤„ç†æ­¥éª¤ï¼šåŒ…å«â€œæ–‡æœ¬é¢„å¤„ç†å®Œæˆï¼ˆæ—¶é—´æˆ³ï¼‰â†’ AI åˆ†æå®Œæˆâ€çš„æµæ°´ï¼›
- å®Œæˆæ—¶é—´ï¼šæœ¬æ¬¡å¤„ç†çš„ç»“æŸæ—¶é—´æˆ³ã€‚

é€šè¿‡ Inspector è°ƒç”¨æ–°å¢çš„ MCP æœåŠ¡å™¨ï¼Œæ³¨æ„éœ€è¦è°ƒæ•´è¶…æ—¶æ—¶é—´

![](/static/images/2025-09-02-langgraph-concept-mcp/iShot_2025-09-03_22.03.56.png)

### è¿è¡Œç»“æœè¯´æ˜
- æœåŠ¡å™¨åœ°å€ï¼šhttp://127.0.0.1:8004/mcpï¼ˆHTTP ä¼ è¾“ï¼ŒFastMCP ä¼šè¾“å‡ºå¯åŠ¨æ—¥å¿—ï¼‰ã€‚
- å¯ç”¨å·¥å…·ï¼šprocess_text_with_langgraphï¼ˆç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ä¸ AI åˆ†æï¼‰ã€‚
- æˆåŠŸè°ƒç”¨åï¼šåœ¨ Inspector ä¸­å¯çœ‹åˆ°è¯¥å·¥å…·çš„å“åº”æ­£æ–‡ï¼ŒåŒ…å«â€œåŸå§‹æ–‡æœ¬ / AI åˆ†æ / å¤„ç†æ­¥éª¤ / å®Œæˆæ—¶é—´â€ç­‰å­—æ®µï¼›è‹¥è¾“å…¥è¾ƒé•¿æ–‡æœ¬ï¼Œå¤„ç†æ­¥éª¤ä¼šæ˜¾ç¤ºå¸¦æ—¶é—´æˆ³çš„è¿›åº¦ä¿¡æ¯ï¼Œä¾¿äºæ’æŸ¥ä¸å¤ç°ã€‚

## ç»“è¯­
æœ¬æ–‡ä»â€œå®¢æˆ·ç«¯é›†æˆâ€å’Œâ€œæœåŠ¡ç«¯å°è£…â€ä¸¤ä¸ªç»´åº¦ï¼Œå±•ç¤ºäº† LangGraph ä¸ FastMCP 2.0 çš„åä½œæ–¹å¼ï¼šå®¢æˆ·ç«¯ä¾§å¯æŒ‰éœ€è°ƒåº¦ MCP å·¥å…·ï¼ŒæœåŠ¡ç«¯ä¾§å¯å°† LangGraph å·¥ä½œæµä»¥ MCP æ ‡å‡†å¯¹å¤–å¤ç”¨ã€‚

## èµ„æ–™

* [ä½¿ç”¨ MCP](https://github.langchain.ac.cn/langgraph/agents/mcp/)


