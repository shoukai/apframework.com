---
title: 生成式AI入门与AWS实战 阅读笔记
date: '2024-12-22'
tags: ['AI', 'Generative AI', '生成式AI', 'LLM', '乱翻书', '读书笔记']
draft: false
summary: '《生成式AI入门与AWS实战》（Generative AI on AWS） 的阅读笔记'
---

![image.png](/static/images/2024-12-24-Generative-AI-on-AWS/image.png)

翻开《生成式AI入门与AWS实战》，看到一众推荐者名单，加上是 O'Reilly 图书，知道这是本需要多翻几遍并进行整理的书。借鉴《好好学习》一书中的五星笔记法，进行拆解。

## 1. **预判核心内容**‌

五星笔记法第一步：预测所学知识的核心所在，思考为什么要讨论这个内容。

### 作者方面

- Chris Fregly：AWS生成式AI首席解决方案架构师
- Antje Barth：AWS生成式AI首席开发倡导者
- Shelbee Eigenbrode：AWS生成式AI首席解决方案架构师

几位作者有着 AI、AWS、机器学习、大数据经验，都有工程师背景。加上书籍标题有 AWS 字样，会有实战案例。

### 时效性方面

书籍时效性方面，中文版是 2024 年 5 月，翻了下英文版的出版信息，2023 年 12 月，加上出版周期，时效一年以上。

![image.png](/static/images/2024-12-24-Generative-AI-on-AWS/image%201.png)

英文版副标题：Building Context-Aware Multimodal Reasoning Applications （构建上下文感知多模态推理应用程序），同样验证了书籍会有理论和实践部分，阅读后也发现的确如此。

### 内容介绍

生成式 AI 项目的生命周期，包括用例定义、模型选择、模型微调、检索增强生成、基于人类反馈的强化学习，以及模型量化、优化和部署。

> You'll learn the generative AI project life cycle including use case definition, model selection, model fine-tuning, retrieval-augmented generation, reinforcement learning from human feedback, and model quantization, optimization, and deployment. And you'll explore different types of models including large language models (LLMs) and multimodal models such as Stable Diffusion for generating images and Flamingo/IDEFICS for answering questions about images.
> 

## **2. 记录讲解逻辑**‌

五星笔记法第二步：记录下知识的逻辑链条，构建出完整的知识体系。

一本精心编写的书籍，会在前言、目录或者结尾，整理出书籍的逻辑脉络或者行文主线，本书在前言中进行了介绍。

> 首先，你将深入探索生成式AI项目生命周期，并学习提示工程、少样本上下文学习、生成式模型预训练、领域自适应、模型评估、参数高效微调（Parameter-Efficient Fine-Tuning，PEFT）和基于人类反馈的强化学习（Reinforcement Learningfrom Human Feedback，RLHF）等主题。
其次，你将动手实践流行的大语言模型，如Llama 2和Falcon，以及包括Stable Diffusion和IDEFICS在内的多模态模型。你可以通过Hugging Face模型中心、Amazon SageMaker JumpStart或AmazonBedrock托管的生成式AI服务访问这些基础模型。 
然后，你还将学习如何实现基于上下文的检索增强生成（Retrieval-Augmented Generation，RAG）和基于agent的推理工作流。你将探索应用程序框架和库，包括LangChain、ReAct和程序辅助语言（Program-Aided-Language，PAL）。你可以利用这些框架和库来访问自己的定制数据源和API，或与外部数据源（如网络搜索和合作伙伴数据系统）进行集成。
最后，你还将在多模态（如文本、图像、音频和视频）生成式AI用例中探索与生成式相关的概念、框架和库。
> 

第一部分为前七章进行了概念讲解，第二部分为第八章讲解流行的大语言模型，第三部分为第九章讲 RAG，最后为多模态等相关内容。

下面是整理出的一些问题域和应对问题出现的技术及原理，个人理解限制，可能存在偏差和不全。

### 指令微调（instruction fine-tuning）

1. 指令微调问题域和定义

> 基础模型已经在数百万份文档、图像、视频和音频片段上进行了预训练，因此它们已经学会人类语言的基础知识，包括类似人类的推理。
基础模型通常还是需要额外的数据或指令，以帮助它们更好地学习特定数据集或领域的知识，并学习如何执行类似人类的任务并逐步推理。这种额外的训练被称为微调，准确说是指令微调。
> 
1. 指令微调的方法

> 指令微调是一种监督式机器学习，通过不断比较给定输入（例如带有对话的指令提示）的模型输出与基准真实标签（例如人工基准摘要）以改进模型。
> 

微调过程：

- 微调大语言模型；
- 生成针对指令提示的回复；
- 计算和基准真实标签之间的差异；
- 更新模型权重；

循环以上四个步骤；

1. 指令数据集问题域

> 如果在微调过程只为单一任务（例如摘要）提供指令，模型可能会出现“灾难性遗忘”的问题，即模型在单一任务上表现太好，以致失去处理其他任务的能力及泛化能力。
> 

### 参数高效微调（Parameter-Eff icient Fine-Tuning，PEFT）

参数高效微调问题域

> 在进行全量微调时，所需的内存资源，例如GPU显存，不仅用于模型本身的存储，也用于训练过程中其他必需参数的存储。与全量微调相比，参数高效微调（Parameter-Eff icient Fine-Tuning，PEFT）技术可以帮助我们利用较少的计算资源来进行模型微调。
> 

PEFT常用技术

> LoRA是一种常用的PEFT技术，它会冻结大语言模型的原始权重，并在Transformer的每一层创建新的、可训练的低秩矩阵。
> 

LoRA 工作原理

> 作为一种微调策略，LoRA通过冻结所有原始模型参数并在模型中的目标模块（如网络层）的原始权重旁插入一对低秩分解矩阵（通常是包括自注意力在内的线性层），来减少需要训练的参数数量
> 

### 基于人类反馈的强化学习（ReinforcementLearning from Human Feedback，RLHF）

RLHF 问题域

> 模型使用互联网中的大量文本数据进行训练。由于这些数据包含大量不良语言和有害性，因此模型生成不符合期望的补全并不奇怪。
> 

RLHF 定义

> 基于人类反馈的强化学习（ReinforcementLearning from Human Feedback，RLHF）是一种微调机制，它使用人类注释（也称为人类反馈）来帮助模型对齐人类价值观和偏好。通常在其他形式的微调（包括指令微调）之后应用RLHF。
> 

### 检索增强生成（Retrieval-Augmented Generation，RAG）

**RAG 的问题域**

幻觉和知识截断。

- 幻觉：模型自信地生成不正确的响应。
- 知识截断（knowledge cutoff）是指模型返回的答案与最新数据不符。

**RAG 如何解决**

- 幻觉方面：RAG 能访问它在预训练和微调过程中学到的“记忆”之外的数据，通过允许模型访问此类信息，可以改善模型补全的相关性，缓解幻觉问题。
- 知识截断：利用 RAG 来访问模型训练日期之后的信息。

**RAG 的实现方案**

RAG架构通常需要考虑两大模块：从外部知识源准备数据的模块和将数据和应用程序集成的模块；

1. 外部知识源准备数据的模块

分为如下步骤：

- 通过嵌入模型为文档生成嵌入向量；
- 嵌入向量存储在向量数据库中；

书籍外延展，从外部知识源准备数据的模块，细分为以下几个步骤：

- 数据源整理：选择符合用例要求的企业结构化和非结构化数据存储库的相关子集；
- 数据清理：对原始数据进行清理，去除不相关的内容、过时的信息和重复数据，以减少模型在检索时的噪音；
- 隐私/PII处理：处理企业数据集中的敏感和私人信息，确保适当处理PII元素，并在适当的情况下进行检测、过滤、编辑和用合成数据替换，以保护隐私并防止潜在的合规性问题；
- 数据提取：从多种格式的数据源（如PDF、Word、数据库等）中提取信息，并进行过滤、压缩、格式化等预处理步骤；
- 文档切片：将长篇文档分割成多个文本块，以便更高效地处理和检索信息，提高信息检索的准确性；
- 向量化：使用嵌入模型将文本转换为向量形式，这种转换使得我们能够通过计算向量之间的差异来识别语义上相似的句子；
- 数据入库：将生成的向量存储在向量数据库中，以便于日后检索信息；

其他：嵌入模型需要与大语言模型紧密匹配，如果大语言模型使用的是本地的Llama 3，那么嵌入模型可以使用LLM2Vec将Llama 3转换为文本嵌入模型。

1. 数据和应用程序集成的模块

从文档中生成嵌入并将其存放到向量数据库中后，应用程序就能够执行检索并找到相关信息。RAG架构后续的工作流会在调用大语言模型前将检索到的信息作为额外的上下文进行提示增强。

在检索到相关的上下文信息后，RAG工作流的下一步就是使用检索到的这些数据进行提示增强。

**RAG 和微调的关系**

> RAG和微调可以一起使用，它们并不互斥。
> 

### Agent 的 ReAct 框架和 PAL 框架

Agent 问题域

> 设想一个基于生成式AI构建的旅行应用程序，它不仅能够针对问题给出一系列建议，还可以为你预订机票和酒店。想要实现以上功能，你需要一个额外的软件，通常称为agent。
> 

ReAct框架

> ReAct 提示策略包含基于思维链的推理和行动计划。ReAct的结构化提示是一个序列，其中包含一个或多个问题（question）、思考（thought）、行动（action）和观察（observation）
> 

PAL 框架

> PAL 首先通过思维链推理在中间推理步骤中生成一些程序，以帮助你解决给定的问题。然后，这些程序被传递给解释器（如Python解释器），以运行代码并将结果返回给基础模型
> 

## 3. **记下疑问和启发**

五星笔记法第三步：在学习过程中记录疑问和突发的灵感

### 提示工程技巧

提示工程，构造高效的提示既是一门艺术，也是一门科学，以下是一些个人关注和经常遇到的建议记录，更多内容见原文，也可以看[《**提示工程指南**》](https://www.promptingguide.ai/zh) ，内容更加丰富。

- 保持清晰和简洁并清晰明了地表达主题；
- 如果文本量大，请将指令移至提示的尾部；
- 使用明确的指令：如果你希望模型以特定格式输出，请直接指定；
- 避免使用否定表达；
- 包括上下文和少样本样例；
- 指定响应的大小、指定的响应格式；
- 要求模型逐步思考：划分子任务、一步一步推理问题等。

### 3.2 推理配置参数

`top-k` 和 `top-p`

- `top-k`是将模型的选择范围限定在概率最高的前 k 个token之内，并从中随机抽取一个token。
- `top-p`限定了模型只能在累积概率不超过某个特定值 p 的token集合中进行随机采样，这会从概率最高的token开始，依次考虑概率较低的token。

仍然不是很直观，通过 kimi 得到了一个更加直观的例子如下：

想象一下，你是一个厨师，面前有一大堆食材，你需要从中选择一些来制作一道菜。这里的食材就相当于模型生成的单词候选，而`top-k`和`top-p`就是你选择食材的策略。

- **Top-k 采样**：
    - 想象你有一个篮子，你只会从食材中挑选出最美味的前`k`种食材放入篮子里。比如，`k=5`，你就只挑选出最美味的5种食材。
    - 然后，你闭上眼睛，从这5种食材中随机挑选一种来制作你的菜。这样，你每次做菜时，虽然选择有限，但至少都是最美味的食材之一。
- **Top-p采样**：
    - 这次，你不是挑选固定数量的食材，而是设定一个标准，比如你只选择那些加起来至少让你觉得90%满意的食材。这就像是`top-p=0.9`，你会选择那些概率累积到90%的食材。
    - 假设你面前有10种食材，你可能会选择前3种（因为它们的累积概率达到了90%），然后从这3种食材中随机挑选一种来做菜。这样，你每次做菜时，选择的食材都是你相对满意的，但具体是哪一种，还有随机性。

这两种策略都旨在从一大堆可能的食材（单词）中做出选择，但`top-k`是固定数量的选择，而`top-p`是基于概率阈值的动态选择。通过调整`k`和`p`的值，你可以控制做菜（生成文本）的多样性和创造性。

## 4. **提炼思维模型**

五星笔记法第四步：从所学知识中提炼出有效的思维模型或策略。

广泛且深入地覆盖了生成式AI的基础概念、核心理论与实践过程。从基础知识出发，步步深入至模型的训练、微调和部署，直至创新应用的各个阶段。将理论知识、实践代码与真实案例相结合，为在 AWS 上构建和部署生成式AI应用程序的开发者提供了一条构建生成式AI解决方案的有效路径。

## 5. **变成具体行动**‌

Generative AI on AWS 一书的代码示例：https://github.com/generative-ai-on-aws/generative-ai-on-aws